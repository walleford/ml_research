{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HQymv06B9xS",
        "outputId": "7eb6f58b-b8db-4038-a6a4-66faa0251d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /datasets/\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount(\n",
        "    '/datasets/'\n",
        ")\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import imblearn\n",
        "import pandas as pd\n",
        "import os\n",
        "np.random.seed(0)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsw_testing_df = pd.read_csv('/datasets/MyDrive/datasets/UNSW_NB15_testing-set.csv')\n",
        "unsw_train_df = pd.read_csv('/datasets/MyDrive/datasets/UNSW_NB15_training-set.csv')\n",
        "cicids_files = ['/datasets/MyDrive/datasets/Wednesday-workingHours.pcap_ISCX.csv',\n",
        "                '/datasets/MyDrive/datasets/Tuesday-WorkingHours.pcap_ISCX.csv',\n",
        "                '/datasets/MyDrive/datasets/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n",
        "                '/datasets/MyDrive/datasets/Monday-WorkingHours.pcap_ISCX.csv'\n",
        "                ]\n",
        "cicids_df = pd.concat((pd.read_csv(f) for f in cicids_files), ignore_index=True)"
      ],
      "metadata": {
        "id": "-Cn-9Hn4CEvm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(df):\n",
        "    categorical_cols = ['proto','service','state']\n",
        "    for col in categorical_cols:\n",
        "        dummies = pd.get_dummies(df[col].astype({col: 'str'}),prefix=col, dtype=int)\n",
        "        df = pd.concat([df,dummies],axis=1)\n",
        "        df = df.drop(col,axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def scaling(df, df_columns):\n",
        "    \"\"\"\n",
        "        This will be used to scale the data in the df to [0,1].\n",
        "\n",
        "        Will be done using the Min-max feature scaling technique\n",
        "        to bring all the values into the range [0,1]\n",
        "    \"\"\"\n",
        "    new_normalized_df = df.copy()\n",
        "    for column in df_columns:\n",
        "        max_value = df[column].max()\n",
        "        min_value = df[column].min()\n",
        "        if max_value > min_value:\n",
        "            new_normalized_df[column] = (new_normalized_df[column] - min_value) / (max_value - min_value)\n",
        "\n",
        "    return new_normalized_df\n",
        "\n",
        "def reduce_mem_usage(props):\n",
        "    start_mem_usg = props.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
        "    NAlist = [] # Keeps track of columns that have missing values filled in.\n",
        "    for col in props.columns:\n",
        "        if props[col].dtype != object:  # Exclude strings\n",
        "\n",
        "            # make variables for Int, max and min\n",
        "            IsInt = False\n",
        "            mx = props[col].max()\n",
        "            mn = props[col].min()\n",
        "\n",
        "            # Integer does not support NA, therefore, NA needs to be filled\n",
        "            if not np.isfinite(props[col]).all():\n",
        "                NAlist.append(col)\n",
        "                props[col].fillna(mn-1,inplace=True)\n",
        "\n",
        "            # test if column can be converted to an integer\n",
        "            asint = props[col].fillna(0).replace([np.inf, -np.inf], 0).astype(np.int64) # Replace inf values with 0\n",
        "            result = (props[col] - asint)\n",
        "            result = result.sum()\n",
        "            if result > -0.01 and result < 0.01:\n",
        "                IsInt = True\n",
        "\n",
        "\n",
        "            # Make Integer/unsigned Integer datatypes\n",
        "            if IsInt:\n",
        "                if mn >= 0:\n",
        "                    if mx < 255:\n",
        "                        props[col] = props[col].astype(np.uint8)\n",
        "                    elif mx < 65535:\n",
        "                        props[col] = props[col].astype(np.uint16)\n",
        "                    elif mx < 4294967295:\n",
        "                        props[col] = props[col].astype(np.uint32)\n",
        "                    else:\n",
        "                        props[col] = props[col].astype(np.uint64)\n",
        "                else:\n",
        "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
        "                        props[col] = props[col].astype(np.int8)\n",
        "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
        "                        props[col] = props[col].astype(np.int16)\n",
        "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
        "                        props[col] = props[col].astype(np.int32)\n",
        "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
        "                        props[col] = props[col].astype(np.int64)\n",
        "\n",
        "            # Make float datatypes 32 bit\n",
        "            else:\n",
        "                props[col] = props[col].astype(np.float32)\n",
        "\n",
        "    # Print final result\n",
        "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
        "    mem_usg = props.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
        "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
        "    return props, NAlist"
      ],
      "metadata": {
        "id": "Ut3mmAjWVHym"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat = unsw_train_df.select_dtypes(exclude=[np.number])\n",
        "print(df_cat.describe(include='all'))\n",
        "DEBUG = 0\n",
        "## reducing the amount of uniques in each feature\n",
        "for feature in df_cat.columns:\n",
        "    if DEBUG == 1:\n",
        "        print(feature)\n",
        "        print('nunique = '+str(df_cat[feature].nunique()))\n",
        "        print(df_cat[feature].nunique()>7)\n",
        "        print(sum(unsw_train_df[feature].isin(unsw_train_df[feature].value_counts().head().index)))\n",
        "        print('----------------------------------------------------')\n",
        "\n",
        "    if df_cat[feature].nunique()>8:\n",
        "        unsw_train_df[feature] = np.where(unsw_train_df[feature].isin(unsw_train_df[feature].value_counts().head().index), unsw_train_df[feature], 'Combined')\n",
        "# taking out the attack category\n",
        "unsw_attack_cat = unsw_train_df.pop('attack_cat')\n",
        "\n",
        "# going to reduce the amount of memory the dataset takes\n",
        "unsw_train_df.columns = unsw_train_df.columns.str.strip()\n",
        "unsw_df, NAList = reduce_mem_usage(unsw_train_df)\n",
        "# replacing infinity, and neg infinity values with NaN then dropping them\n",
        "unsw_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "unsw_df.dropna(inplace=True)\n",
        "# dropping duplicates\n",
        "print(f\"Fully duplicate rows to drop: {unsw_df.duplicated().sum()}\")\n",
        "unsw_df.drop_duplicates(inplace=True)\n",
        "unsw_df.reset_index(drop=True, inplace=True)\n",
        "# one hot encoding the entire dataframe\n",
        "unsw_e_df = one_hot(unsw_df)\n",
        "# scaling the dataframe using our scaling() function\n",
        "unsw_s_df = scaling(unsw_e_df, unsw_e_df.columns)\n",
        "# dropping the label column in order to undersample\n",
        "unsw_unlabled_df = unsw_s_df.drop('label', axis=1)\n",
        "# setting the labels to a var so we don't lose them\n",
        "lables = unsw_s_df['label']\n",
        "print(f\"UNSW Dataframe Shape After cleaning: {unsw_df.shape}\")\n",
        "# splitting the dataframe into testing and training with a 75/25 split\n",
        "unsw_unlabled_df_train, unsw_unlabeled_df_test, unsw_lables_train, unsw_lables_test = train_test_split(unsw_unlabled_df, lables, train_size=0.75, random_state=42)\n",
        "# under-sampling the dataset using SMOTE\n",
        "under = RandomUnderSampler(sampling_strategy=1)\n",
        "unsw_train_smote, unsw_label_train_smote = under.fit_resample(unsw_unlabled_df_train, unsw_lables_train)\n",
        "# adding the labels back to the dataframe\n",
        "unsw_train_df_smote = pd.concat([unsw_train_smote, unsw_label_train_smote], axis=1)\n",
        "# converting to a numpy array so we can reshape the input for our models\n",
        "unsw_nump_train = unsw_train_smote.to_numpy()\n",
        "unsw_nump_test = unsw_unlabeled_df_test.to_numpy()\n",
        "unsw_train = unsw_nump_train.reshape(unsw_nump_train.shape[0], 1, unsw_nump_train.shape[1])\n",
        "unsw_test = unsw_nump_test.reshape(unsw_nump_test.shape[0], 1, unsw_nump_test.shape[1])\n",
        "print(f\"UNSW Testing Sample Shape: {unsw_test.shape}\")\n",
        "print(f\"UNSW Training Sample Shape: {unsw_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM-jJb1GVZhc",
        "outputId": "1ace35a0-0874-4d44-ba9a-022e6325e949",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         proto service   state attack_cat\n",
            "count   175341  175341  175341     175341\n",
            "unique     133      13       9         10\n",
            "top        tcp       -     INT     Normal\n",
            "freq     79946   94168   82275      56000\n",
            "Memory usage of properties dataframe is : 58.860931396484375  MB\n",
            "___MEMORY USAGE AFTER COMPLETION:___\n",
            "Memory usage is:  20.902398109436035  MB\n",
            "This is  35.51149737784218 % of the initial size\n",
            "Fully duplicate rows to drop: 0\n",
            "UNSW Dataframe Shape After cleaning: (175341, 44)\n",
            "UNSW Testing Sample Shape: (43836, 1, 58)\n",
            "UNSW Training Sample Shape: (84058, 1, 58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preliminary Work on CICIDS Dataset\n",
        "\n",
        "  To simplify the modeling, I am going to create a mal_or_not column. 0 is for benign and 1 is for malicious. Then I will just drop the Label column"
      ],
      "metadata": {
        "id": "Afd4Nr_8egli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"CICIDS Dataframe Shape Before: {cicids_df.shape}\")\n",
        "cicids_df.loc[cicids_df[' Label'] != \"BENIGN\", 'mal_or_not'] = 1\n",
        "cicids_df.loc[cicids_df[' Label'] == \"BENIGN\", 'mal_or_not'] = 0\n",
        "# pop the label off, then scale as it is a categorical column\n",
        "attack_labels = cicids_df.pop(\" Label\")\n",
        "drop_columns = [ # this list includes all spellings across CIC NIDS datasets\n",
        "    \"Flow ID\",\n",
        "    'Fwd Header Length.1',\n",
        "    \"Source IP\", \"Src IP\",\n",
        "    \"Source Port\", \"Src Port\",\n",
        "    \"Destination IP\", \"Dst IP\",\n",
        "    \"Destination Port\", \"Dst Port\",\n",
        "    \"Timestamp\",\n",
        "]\n",
        "temp = cicids_df\n",
        "temp.columns = temp.columns.str.strip() # deleting the trailing whitespaces if there are any\n",
        "temp.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
        "temp_df, NAList = reduce_mem_usage(temp)\n",
        "temp_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "temp_df.dropna(inplace=True)\n",
        "print(f\"fully duplicate rows to remove: {temp_df.duplicated().sum()}\")\n",
        "temp_df.drop_duplicates(inplace=True)\n",
        "temp_df.reset_index(drop=True, inplace=True)\n",
        "cicids_df = temp_df\n",
        "# scaling the dataframe using our scaling() function\n",
        "cicids_s_df = scaling(cicids_df, cicids_df.columns)\n",
        "# dropping the label column in order to undersample\n",
        "cicids_unlabled_df = cicids_s_df.drop('mal_or_not', axis=1)\n",
        "# setting the labels to a var so we don't lose them\n",
        "lables = cicids_s_df['mal_or_not']\n",
        "print(f\"CICIDS Dataframe Shape After cleaning: {cicids_df.shape}\")\n",
        "# splitting the dataframe into testing and training with a 75/25 split\n",
        "cicids_unlabled_df_train, cicids_unlabeled_df_test, cicids_lables_train, cicids_lables_test = train_test_split(cicids_unlabled_df, lables, train_size=0.75, random_state=42)\n",
        "# under-sampling the dataset using SMOTE\n",
        "under = RandomUnderSampler(sampling_strategy=1)\n",
        "cicids_train_smote, cicids_label_train_smote = under.fit_resample(cicids_unlabled_df_train, cicids_lables_train)\n",
        "# adding the labels back to the dataframe\n",
        "cicids_train_df_smote = pd.concat([cicids_train_smote, cicids_label_train_smote], axis=1)\n",
        "# converting to a numpy array so we can reshape the input for our models\n",
        "cicids_nump_train = cicids_train_smote.to_numpy()\n",
        "cicids_nump_test = cicids_unlabeled_df_test.to_numpy()\n",
        "cicids_train = cicids_nump_train.reshape(cicids_nump_train.shape[0], 1, cicids_nump_train.shape[1])\n",
        "cicids_test = cicids_nump_test.reshape(cicids_nump_test.shape[0], 1, cicids_nump_test.shape[1])\n",
        "print(f\"CICIDS Testing Sample Shape: {cicids_test.shape}\")\n",
        "print(f\"CICIDS Training Sample Shape: {cicids_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHvUKrhtZE8l",
        "outputId": "22f66ccf-2e77-431e-8746-41329ca05286",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CICIDS Dataframe Shape Before: (1859563, 79)\n",
            "Memory usage of properties dataframe is : 1092.4252853393555  MB\n",
            "___MEMORY USAGE AFTER COMPLETION:___\n",
            "Memory usage is:  425.6203155517578  MB\n",
            "This is  38.96104578168395 % of the initial size\n",
            "fully duplicate rows to remove: 300645\n",
            "CICIDS Dataframe Shape After cleaning: (1556798, 77)\n",
            "CICIDS Testing Sample Shape: (389200, 1, 76)\n",
            "CICIDS Training Sample Shape: (306398, 1, 76)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def list_different_columns(df1, df2):\n",
        "    \"\"\"\n",
        "    List the different columns between two pandas DataFrames.\n",
        "\n",
        "    Parameters:\n",
        "    - df1: pandas DataFrame, first DataFrame.\n",
        "    - df2: pandas DataFrame, second DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary with keys 'unique_to_df1' and 'unique_to_df2' containing the column names unique to each DataFrame.\n",
        "    \"\"\"\n",
        "    # Extract column names\n",
        "    columns_df1 = set(df1.columns)\n",
        "    columns_df2 = set(df2.columns)\n",
        "\n",
        "    # Find columns unique to each DataFrame\n",
        "    unique_to_df1 = columns_df1 - columns_df2\n",
        "    unique_to_df2 = columns_df2 - columns_df1\n",
        "\n",
        "    return {\n",
        "        'unique_to_df1': list(unique_to_df1),\n",
        "        'unique_to_df2': list(unique_to_df2)\n",
        "    }\n",
        "different_columns = list_different_columns(cicids_unlabeled_df_test, cicids_train_smote)\n",
        "\n",
        "print(\"Columns unique to test:\", different_columns['unique_to_df1'])\n",
        "print(\"Columns unique to train:\", different_columns['unique_to_df2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZsh-QoXYWot",
        "outputId": "2e8119dd-18d1-4107-d195-43d4d0492ca1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns unique to test: []\n",
            "Columns unique to train: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now to build the MLP Deep Learning Network"
      ],
      "metadata": {
        "id": "FwFAODYAk_jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cicids_model = keras.Sequential([\n",
        "    keras.layers.Dense(64, input_shape=(cicids_train.shape[1], cicids_train.shape[2])),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "qGgd-meGlDJ1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cicids_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "M7TmUmVpmalU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cicids_res = cicids_model.fit(cicids_train, cicids_label_train_smote, epochs=10, validation_split=0.1, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAM_ySVqmdnm",
        "outputId": "b3a3f3e9-a2c6-4dec-8d6e-8e011dc9f43c",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4309/4309 [==============================] - 19s 4ms/step - loss: 0.1022 - accuracy: 0.9539 - val_loss: 0.1373 - val_accuracy: 0.9181\n",
            "Epoch 2/10\n",
            "4309/4309 [==============================] - 17s 4ms/step - loss: 0.0707 - accuracy: 0.9665 - val_loss: 0.0839 - val_accuracy: 0.9457\n",
            "Epoch 3/10\n",
            "4309/4309 [==============================] - 16s 4ms/step - loss: 0.0602 - accuracy: 0.9727 - val_loss: 0.0537 - val_accuracy: 0.9920\n",
            "Epoch 4/10\n",
            "4309/4309 [==============================] - 17s 4ms/step - loss: 0.0547 - accuracy: 0.9763 - val_loss: 0.0548 - val_accuracy: 0.9920\n",
            "Epoch 5/10\n",
            "4309/4309 [==============================] - 16s 4ms/step - loss: 0.0508 - accuracy: 0.9778 - val_loss: 0.0517 - val_accuracy: 0.9940\n",
            "Epoch 6/10\n",
            "4309/4309 [==============================] - 16s 4ms/step - loss: 0.0496 - accuracy: 0.9784 - val_loss: 0.0563 - val_accuracy: 0.9910\n",
            "Epoch 7/10\n",
            "4309/4309 [==============================] - 16s 4ms/step - loss: 0.0474 - accuracy: 0.9795 - val_loss: 0.0470 - val_accuracy: 0.9926\n",
            "Epoch 8/10\n",
            "4309/4309 [==============================] - 17s 4ms/step - loss: 0.0456 - accuracy: 0.9803 - val_loss: 0.0384 - val_accuracy: 0.9914\n",
            "Epoch 9/10\n",
            "4309/4309 [==============================] - 17s 4ms/step - loss: 0.0444 - accuracy: 0.9807 - val_loss: 0.0570 - val_accuracy: 0.9848\n",
            "Epoch 10/10\n",
            "4309/4309 [==============================] - 17s 4ms/step - loss: 0.0443 - accuracy: 0.9809 - val_loss: 0.0500 - val_accuracy: 0.9920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the metrics for the current model"
      ],
      "metadata": {
        "id": "WtuPu7AQkk1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(lables, predictions):\n",
        "  true_lables = lables.values.flatten()\n",
        "  predicted_labels = predictions.flatten()\n",
        "\n",
        "  accuracy = accuracy_score(true_lables, predicted_labels)\n",
        "  precision = precision_score(true_lables, predicted_labels)\n",
        "  recall = recall_score(true_lables, predicted_labels)\n",
        "  f1 = f1_score(true_lables, predicted_labels)\n",
        "  print(f'Accuracy: {accuracy:.2f}')\n",
        "  print(f'Precision: {precision:.2f}')\n",
        "  print(f'Recall: {recall:.2f}')\n",
        "  print(f'F1 Score: {f1:.2f}')"
      ],
      "metadata": {
        "id": "hZaX2nIgl1xe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "def make_predictions(model, data, labels):\n",
        "    predictions = model.predict(data)\n",
        "    binary_predictions = np.where(predictions >= 0.5, 1, 0)\n",
        "    return binary_predictions\n",
        "\n",
        "clean_preds = make_predictions(cicids_model, cicids_test, cicids_lables_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYu_QH_jkAK-",
        "outputId": "bac7731b-ec26-4cb7-d2fe-ab48845012ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12163/12163 [==============================] - 31s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_metrics(cicids_lables_test, clean_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Z1CEUomE3r",
        "outputId": "2b0ea462-bb84-4e9e-b604-a1b0a42db36f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.98\n",
            "Precision: 0.85\n",
            "Recall: 0.99\n",
            "F1 Score: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building and Modeling the same model with the UNSW dataset"
      ],
      "metadata": {
        "id": "Kxz7jnJqRBrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unsw_model = keras.Sequential([\n",
        "    keras.layers.Dense(64, input_shape=(unsw_train.shape[1], unsw_train.shape[2])),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(32, activation='relu'),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "-Fyq8wq8fFsS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unsw_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=keras.losses.BinaryCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "QujnvK5kRHc9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unsw_res = unsw_model.fit(unsw_train, unsw_label_train_smote, epochs=10, validation_split=0.1, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Xc8e13RH3F",
        "outputId": "7ed7e923-9dc4-4484-a8ff-b8539c5864d2",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1183/1183 [==============================] - 6s 4ms/step - loss: 0.1489 - accuracy: 0.9307 - val_loss: 0.1336 - val_accuracy: 0.9333\n",
            "Epoch 2/10\n",
            "1183/1183 [==============================] - 5s 4ms/step - loss: 0.1181 - accuracy: 0.9457 - val_loss: 0.1894 - val_accuracy: 0.9014\n",
            "Epoch 3/10\n",
            "1183/1183 [==============================] - 5s 4ms/step - loss: 0.1104 - accuracy: 0.9482 - val_loss: 0.1107 - val_accuracy: 0.9415\n",
            "Epoch 4/10\n",
            "1183/1183 [==============================] - 4s 4ms/step - loss: 0.0976 - accuracy: 0.9537 - val_loss: 0.1670 - val_accuracy: 0.9161\n",
            "Epoch 5/10\n",
            "1183/1183 [==============================] - 5s 4ms/step - loss: 0.0852 - accuracy: 0.9598 - val_loss: 0.0893 - val_accuracy: 0.9532\n",
            "Epoch 6/10\n",
            "1183/1183 [==============================] - 4s 4ms/step - loss: 0.0798 - accuracy: 0.9622 - val_loss: 0.0791 - val_accuracy: 0.9579\n",
            "Epoch 7/10\n",
            "1183/1183 [==============================] - 4s 4ms/step - loss: 0.0736 - accuracy: 0.9653 - val_loss: 0.1914 - val_accuracy: 0.9367\n",
            "Epoch 8/10\n",
            "1183/1183 [==============================] - 4s 4ms/step - loss: 0.0742 - accuracy: 0.9651 - val_loss: 0.0728 - val_accuracy: 0.9600\n",
            "Epoch 9/10\n",
            "1183/1183 [==============================] - 4s 4ms/step - loss: 0.0697 - accuracy: 0.9674 - val_loss: 0.0625 - val_accuracy: 0.9635\n",
            "Epoch 10/10\n",
            "1183/1183 [==============================] - 4s 4ms/step - loss: 0.0696 - accuracy: 0.9671 - val_loss: 0.0882 - val_accuracy: 0.9518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unsw_clean_preds = make_predictions(unsw_model, unsw_test, unsw_lables_test)\n",
        "calculate_metrics(unsw_lables_test, unsw_clean_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKtjNagXoOvQ",
        "outputId": "1b4c1ee7-6a83-459e-d805-e43995f257b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1370/1370 [==============================] - 4s 3ms/step\n",
            "Accuracy: 0.96\n",
            "Precision: 0.99\n",
            "Recall: 0.95\n",
            "F1 Score: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating and fitting adversarial examples for each model:\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_09aD34ec8Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_obj = keras.losses.BinaryCrossentropy()\n",
        "def create_adversarial_examples(input, labels, model):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(input)\n",
        "    predictions = model(input)\n",
        "    loss = loss_obj(labels, predictions)\n",
        "  gradient = tape.gradient(loss, input)\n",
        "  signed_grad = tf.sign(gradient)\n",
        "  return signed_grad"
      ],
      "metadata": {
        "id": "UwhSsqWJdA1J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CICIDS\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CrO_Zo2AdEu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cicids_test = tf.convert_to_tensor(cicids_test)\n",
        "cicids_lables = tf.reshape(cicids_lables_test, (cicids_lables_test.shape[0], 1))\n",
        "c_adv_x = create_adversarial_examples(cicids_test, cicids_lables, cicids_model) + cicids_test\n",
        "c_adv_preds = make_predictions(cicids_model, c_adv_x, cicids_lables_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSR20b7aQVgE",
        "outputId": "87a3499f-690b-487e-9fcc-95887aa4173a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12163/12163 [==============================] - 33s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_metrics(cicids_lables_test, c_adv_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJx-V6mQTvED",
        "outputId": "d0f67350-1abf-4238-ffe9-f3436cbcb13e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.77\n",
            "Precision: 0.33\n",
            "Recall: 0.76\n",
            "F1 Score: 0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNSW\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vCS-UUIXdKjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming create_adversarial_examples function is defined earlier\n",
        "\n",
        "# Convert the test set and labels to tensors\n",
        "unsw_test = tf.convert_to_tensor(unsw_test)\n",
        "unsw_labels = tf.reshape(unsw_lables_test, (unsw_lables_test.shape[0], 1))\n",
        "\n",
        "# Create adversarial examples\n",
        "u_adv_x = create_adversarial_examples(unsw_test, unsw_labels, unsw_model) + unsw_test\n",
        "\n",
        "# Predict on the adversarial examples\n",
        "u_adv_preds = make_predictions(unsw_model, u_adv_x, unsw_lables_test)\n",
        "calculate_metrics(unsw_lables_test, u_adv_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeFg3UgfZLyp",
        "outputId": "b91c4908-fade-4307-dcb0-1c11b25ad595"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1370/1370 [==============================] - 4s 3ms/step\n",
            "Accuracy: 0.57\n",
            "Precision: 0.83\n",
            "Recall: 0.47\n",
            "F1 Score: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Things"
      ],
      "metadata": {
        "id": "y5hBbkqePvxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fig. 3, the summarization of the MLP Model Trained on the UNSW Dataset\")\n",
        "print(unsw_model.summary())\n",
        "print(\"Fig. 4, the summarization of the MLP Model Trained on the CICIDS Dataset\")\n",
        "print(cicids_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaO5L13KPxBm",
        "outputId": "fda9bec9-5242-45b8-a9a2-d22b527f4953"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fig. 3, the summarization of the MLP Model Trained on the UNSW Dataset\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 1, 64)             3776      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1, 64)             4160      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1, 32)             2080      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1, 16)             528       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1, 1)              17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10561 (41.25 KB)\n",
            "Trainable params: 10561 (41.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Fig. 4, the summarization of the MLP Model Trained on the CICIDS Dataset\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1, 64)             4928      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1, 64)             4160      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1, 32)             2080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1, 16)             528       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1, 1)              17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11713 (45.75 KB)\n",
            "Trainable params: 11713 (45.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix1 = confusion_matrix(cicids_lables_test, clean_preds)\n",
        "conf_matrix2 = confusion_matrix(unsw_lables_test, unsw_clean_preds)\n",
        "conf_matrix3 = confusion_matrix(cicids_lables_test, c_adv_preds)\n",
        "conf_matrix4 = confusion_matrix(unsw_lables_test, u_adv_preds)\n",
        "\n",
        "# Step 5: Plot the confusion matrix using seaborn for better visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Negative', 'Predicted Positive'], yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "g9U00vBTQTq5",
        "outputId": "218eaaf0-8d1c-4e13-cecd-93a46d083c08"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Classification metrics can't handle a mix of binary and unknown targets",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e3d8d6371c93>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconf_matrix1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcicids_lables_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconf_matrix2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsw_lables_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsw_clean_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconf_matrix3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcicids_lables_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_adv_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconf_matrix4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsw_lables_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_adv_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and unknown targets"
          ]
        }
      ]
    }
  ]
}